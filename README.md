# llama-chat-offline--LLaMATE
ğŸ’¬ Offline Chatbot UI using Streamlit + Ollama + LLaMA + Image/Voice/Doc Q&amp;A

An ultra-modern, **fully offline chatbot** UI using [Ollama](https://ollama.com), [LLaMA 3](https://ollama.com/library/llama3), and [Streamlit](https://streamlit.io).

## ğŸš€ Features

ğŸ§  LLaMA-3 (8B) via Ollama â€“ fast local inference, open-weights

ğŸ’¬ Real-time Chat UI â€“ markdown, emoji, avatars, clean layout

ğŸ“„ PDF / TXT Upload â€“ inject document context into the chat

ğŸ–¼ï¸ Image Upload â€“ ask questions based on images (optional)

ğŸ”’ Private & Secure â€“ nothing leaves your device

âš™ï¸ Adjustable AI settings â€“ temperature, topâ€‘p, model selection

ğŸ§¹ One-click Clear Chat / Context

ğŸ¨ Custom dark UI with Streamlit + CSS

ğŸš« No API keys or cloud needed

ğŸ› ï¸ Built With
Python

Streamlit

Ollama

LLaMA

pypdf

Pillow


---

## ğŸ“¦ Run Locally

```bash
# 1. Clone the repo
git clone https://github.com/yourusername/llama-chat-ui-offline.git
cd llama-chat-ui-offline

# 2. Set up Python environment
python -m venv .venv
source .venv/bin/activate        # (Windows: .venv\\Scripts\\activate)

# 3. Install dependencies
pip install -r requirements.txt

# 4. Start Ollama and run model
ollama run llama3

# 5. Launch the app
streamlit run app.py

